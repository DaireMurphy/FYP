{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "FastModel = KeyedVectors.load_word2vec_format('FastText.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model - Large PreTrained Google News bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogleModel = KeyedVectors.load_word2vec_format('Google.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.8543272018432617),\n",
       " ('teenage_girl', 0.7927976250648499),\n",
       " ('woman', 0.7494640946388245),\n",
       " ('teenager', 0.717249870300293),\n",
       " ('schoolgirl', 0.7075953483581543),\n",
       " ('teenaged_girl', 0.6650916337966919),\n",
       " ('daughter', 0.6489864587783813),\n",
       " ('mother', 0.6478164196014404),\n",
       " ('toddler', 0.6473966836929321),\n",
       " ('girls', 0.6154742240905762)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoogleModel.most_similar('girl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placing the results of the most_similar into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1              boy\n",
      "2         teenager\n",
      "4             girl\n",
      "6           robber\n",
      "9              men\n",
      "11             guy\n",
      "12          person\n",
      "15    motorcyclist\n",
      "21         suspect\n",
      "24          victim\n",
      "Name: Most Similar, dtype: object\n"
     ]
    }
   ],
   "source": [
    "manList = GoogleModel.most_similar(positive=['man'], topn=30)\n",
    "word = [\"man\"]\n",
    "string = ' '.join([str(elem) for elem in word]) \n",
    "string = string.lower()\n",
    "dfman = pd.DataFrame(manList, columns = ['Most Similar' , 'Vector Accuracy'])\n",
    "dfman = dfman.apply(lambda x: x.astype(str).str.lower())\n",
    "dfman = dfman[~dfman['Most Similar'].str.contains(\"_\")]\n",
    "dfman = dfman[~dfman['Most Similar'].str.contains(string)]\n",
    "print (dfman['Most Similar'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the most similar words from a list of words using a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Word: Trump\n",
      "0                Donald_Trump\n",
      "1    impersonator_entertained\n",
      "2                Ivanka_Trump\n",
      "3                      Ivanka\n",
      "4          mogul_Donald_Trump\n",
      "5                 Trump_Tower\n",
      "6                     Kepcher\n",
      "7    billionaire_Donald_Trump\n",
      "8                   Trumpster\n",
      "9         tycoon_Donald_Trump\n",
      "Name: Most Similar, dtype: object\n",
      "-- Word: Obama\n",
      "0              Barack_Obama\n",
      "1    President_Barack_Obama\n",
      "2                    McCain\n",
      "3                   Clinton\n",
      "4          Illinois_senator\n",
      "5                     Biden\n",
      "6                      Bush\n",
      "7                    Barack\n",
      "8               White_House\n",
      "9        elect_Barack_Obama\n",
      "Name: Most Similar, dtype: object\n",
      "-- Word: Clinton\n",
      "0            Hillary_Clinton\n",
      "1                      Obama\n",
      "2               Bill_Clinton\n",
      "3     Hillary_Rodham_Clinton\n",
      "4       Sen._Hillary_Clinton\n",
      "5                    Hillary\n",
      "6    Senator_Hillary_Clinton\n",
      "7                     McCain\n",
      "8                   Clintons\n",
      "9               Barack_Obama\n",
      "Name: Most Similar, dtype: object\n"
     ]
    }
   ],
   "source": [
    "words = [\"Trump\", \"Obama\", \"Clinton\" ]\n",
    "for word in words:\n",
    "    print(\"-- Word: %s\" % word)\n",
    "    result = GoogleModel.most_similar(positive=word)\n",
    "    dfloop = pd.DataFrame(result, columns = ['Most Similar' , 'Vector Accuracy'])\n",
    "    print (dfloop['Most Similar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drum', 0.6940563321113586),\n",
       " ('drumming', 0.5132456421852112),\n",
       " ('music', 0.5001342296600342),\n",
       " ('Drum', 0.40797561407089233),\n",
       " ('hip_hop', 0.4001891016960144),\n",
       " ('bands', 0.3928840160369873),\n",
       " ('bhangra', 0.3885735273361206),\n",
       " ('reggae_dancehall', 0.38298097252845764),\n",
       " ('reggae', 0.38292473554611206),\n",
       " ('reggae_hip_hop', 0.3807390630245209)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2 = GoogleModel['music'] - GoogleModel['piano'] + GoogleModel['drum']\n",
    "GoogleModel.most_similar([vec2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Self Trained Reddit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in a csv of a medium size to a dataframe, and sepreating out the required 'Title' column to be used to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>date_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1201232046</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1201232075</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1201232523</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1201233290</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "      <td>False</td>\n",
       "      <td>fadi420</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1201274720</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "      <td>False</td>\n",
       "      <td>mhermans</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_created date_created  up_votes  down_votes  \\\n",
       "0    1201232046   2008-01-25         3           0   \n",
       "1    1201232075   2008-01-25         2           0   \n",
       "2    1201232523   2008-01-25         3           0   \n",
       "3    1201233290   2008-01-25         1           0   \n",
       "4    1201274720   2008-01-25         4           0   \n",
       "\n",
       "                                             title  over_18    author  \\\n",
       "0                Scores killed in Pakistan clashes    False     polar   \n",
       "1                 Japan resumes refuelling mission    False     polar   \n",
       "2                  US presses Egypt on Gaza border    False     polar   \n",
       "3     Jump-start economy: Give health care to all     False   fadi420   \n",
       "4  Council of Europe bashes EU&UN terror blacklist    False  mhermans   \n",
       "\n",
       "   subreddit  \n",
       "0  worldnews  \n",
       "1  worldnews  \n",
       "2  worldnews  \n",
       "3  worldnews  \n",
       "4  worldnews  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfReddit = pd.read_csv('RedditNews.csv')\n",
    "dfReddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509236, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newTitles = dfReddit[\"title\"]\n",
    "dfReddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the newTitles list and print the first 10 objects in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Scores', 'killed', 'in', 'Pakistan', 'clashes'],\n",
       " ['Japan', 'resumes', 'refuelling', 'mission'],\n",
       " ['US', 'presses', 'Egypt', 'on', 'Gaza', 'border'],\n",
       " ['Jump-start', 'economy', ':', 'Give', 'health', 'care', 'to', 'all'],\n",
       " ['Council', 'of', 'Europe', 'bashes', 'EU', '&', 'UN', 'terror', 'blacklist'],\n",
       " ['Hay',\n",
       "  'presto',\n",
       "  '!',\n",
       "  'Farmer',\n",
       "  'unveils',\n",
       "  'the',\n",
       "  'illegal',\n",
       "  'mock-Tudor',\n",
       "  'castle',\n",
       "  'he',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'hide',\n",
       "  'behind',\n",
       "  '40ft',\n",
       "  'hay',\n",
       "  'bales'],\n",
       " ['Strikes',\n",
       "  ',',\n",
       "  'Protests',\n",
       "  'and',\n",
       "  'Gridlock',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Poland-Ukraine',\n",
       "  'Border'],\n",
       " ['The', 'U.N', '.', 'Mismanagement', 'Program'],\n",
       " ['Nicolas', 'Sarkozy', 'threatens', 'to', 'sue', 'Ryanair'],\n",
       " ['US',\n",
       "  'plans',\n",
       "  'for',\n",
       "  'missile',\n",
       "  'shields',\n",
       "  'in',\n",
       "  'Polish',\n",
       "  'town',\n",
       "  'met',\n",
       "  'with',\n",
       "  'resistance',\n",
       "  '[',\n",
       "  'video',\n",
       "  ']']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsVec = [nltk.word_tokenize(title) for title in newTitles]\n",
    "newsVec[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2Vec, assign each word in the model a vector of size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedditModel = Word2Vec(newsVec,min_count=1,size=100, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9013823866844177),\n",
       " ('teenager', 0.8415164947509766),\n",
       " ('boy', 0.810286819934845),\n",
       " ('girl', 0.808212399482727),\n",
       " ('couple', 0.7884554862976074),\n",
       " ('doctor', 0.7665772438049316),\n",
       " ('mother', 0.7430440187454224),\n",
       " ('teacher', 0.7342153787612915),\n",
       " ('policeman', 0.7302061319351196),\n",
       " ('teen', 0.7246348261833191)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RedditModel.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Small Wiki Dump bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "WikiModel = KeyedVectors.load_word2vec_format('wiki.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Most Similar  Vector Accuracy\n",
      "0          boy         0.712060\n",
      "1      destiny         0.702904\n",
      "2         love         0.694018\n",
      "3       lovers         0.688722\n",
      "4        sorry         0.682142\n",
      "5      teenage         0.673685\n",
      "6       beasts         0.667998\n",
      "7        mercy         0.659093\n",
      "8        hello         0.657588\n",
      "9     restless         0.656618\n"
     ]
    }
   ],
   "source": [
    "WikiModel.most_similar('girl')\n",
    "manList = WikiModel.most_similar(positive=['girl'], topn=30)\n",
    "df = pd.DataFrame(manList, columns = ['Most Similar' , 'Vector Accuracy'])\n",
    "print (df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Comparisons of the 3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.8680489659309387),\n",
       " ('puppy', 0.8106428384780884),\n",
       " ('pit_bull', 0.780396044254303),\n",
       " ('pooch', 0.7627377510070801),\n",
       " ('cat', 0.7609456777572632),\n",
       " ('golden_retriever', 0.7500902414321899),\n",
       " ('German_shepherd', 0.7465174198150635),\n",
       " ('Rottweiler', 0.7437614798545837),\n",
       " ('beagle', 0.7418621778488159),\n",
       " ('pup', 0.740691065788269)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoogleModel.most_similar('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.7921118140220642),\n",
       " ('pig', 0.7833889126777649),\n",
       " ('baby', 0.7613813281059265),\n",
       " ('horse', 0.7580305337905884),\n",
       " ('tiger', 0.7519763708114624),\n",
       " ('pet', 0.7497408390045166),\n",
       " ('crocodile', 0.7390192747116089),\n",
       " ('penis', 0.738898754119873),\n",
       " ('bullet', 0.7388013005256653),\n",
       " ('naked', 0.7386067509651184)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RedditModel.wv.most_similar('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.7068554162979126),\n",
       " ('rabbits', 0.687908411026001),\n",
       " ('cat', 0.6614758968353271),\n",
       " ('cats', 0.6565461158752441),\n",
       " ('anthropomorphic', 0.6554036736488342),\n",
       " ('t_shirts', 0.6497201919555664),\n",
       " ('beast', 0.6489152908325195),\n",
       " ('coyote', 0.6422343850135803),\n",
       " ('flesh', 0.6411483883857727),\n",
       " ('beasts', 0.6408825516700745)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WikiModel.most_similar('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Most Similar Google                Most Similar Reddit  \\\n",
      "0         (farms, 0.7599002122879028)        (farms, 0.7457308769226074)   \n",
      "1    (dairy_farm, 0.7567876577377319)         (pool, 0.7318962216377258)   \n",
      "2       (farming, 0.7305764555931091)  (supermarket, 0.7294552326202393)   \n",
      "3        (farmer, 0.7109191417694092)         (mine, 0.7275694012641907)   \n",
      "4  (DUANE_HOWELL, 0.6612095832824707)       (copper, 0.7254145741462708)   \n",
      "5          (Farm, 0.6409205198287964)         (gold, 0.7237563133239746)   \n",
      "6  (agricultural, 0.6379073262214661)        (coffee, 0.721081018447876)   \n",
      "7  (dairy_farmer, 0.6344161033630371)        (sewage, 0.714181661605835)   \n",
      "8       (farmers, 0.6342907547950745)        (steel, 0.6917004585266113)   \n",
      "9   (v._Sos_prov, 0.6199461221694946)       (forest, 0.6916866302490234)   \n",
      "\n",
      "                  Most Similar Wiki  \n",
      "0  (plantation, 0.6966841220855713)  \n",
      "1   (farmhouses, 0.683635950088501)  \n",
      "2       (farms, 0.6830114722251892)  \n",
      "3     (farming, 0.6814944744110107)  \n",
      "4   (farmstead, 0.6687393188476562)  \n",
      "5        (grist, 0.668049693107605)  \n",
      "6  (blacksmith, 0.6589310765266418)  \n",
      "7    (freehold, 0.6574586033821106)  \n",
      "8       (manor, 0.6539639234542847)  \n",
      "9   (maplewood, 0.6505680680274963)  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word = 'farm'\n",
    "result1 = GoogleModel.most_similar(positive=word)\n",
    "result2 = RedditModel.most_similar(positive=word)\n",
    "result3 = WikiModel.most_similar(positive=word)\n",
    "dftable = pd.DataFrame(result2, columns = ['Most Similar' , 'Vector Accuracy'])\n",
    "dftable.insert(0, 'Most Similar Google',  result1, True)\n",
    "dftable.insert(1, 'Most Similar Reddit',  result2, True)\n",
    "dftable.insert(2, 'Most Similar Wiki',  result3, True)\n",
    "dftable = dftable.drop('Most Similar', 1)\n",
    "dftable = dftable.drop('Vector Accuracy', 1)\n",
    "print(dftable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
